{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "import numpy as np\n",
    "from max import engine\n",
    "from max.graph import Graph, TensorType, ops\n",
    "from max.driver import accelerator_count, CPU, CUDA, Device\n",
    "from max.dtype import DType"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See if we have accelerators available through CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have a 1 GPUs available!\n"
     ]
    }
   ],
   "source": [
    "if count := accelerator_count():\n",
    "    print(f'We have a {count} GPUs available!')\n",
    "else:\n",
    "    print('Sorry, CPU only.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gpu'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_device = CPU() if accelerator_count() == 0 else CUDA()\n",
    "my_device.label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensor [addition on the GPU via MAX graphs](https://github.com/modular/max/blob/main/tutorials/max-graph-python/src/max_ops/addition.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_tensors(a: np.ndarray, b: np.ndarray, device: Device) -> dict[str, Any]:\n",
    "    # 1. Build the graph\n",
    "    input_type = TensorType(dtype=DType.float32, shape=(1,))\n",
    "    with Graph(\n",
    "        \"simple_add_graph\", input_types=(input_type, input_type)\n",
    "    ) as graph:\n",
    "        lhs, rhs = graph.inputs\n",
    "        out = ops.add(lhs, rhs)\n",
    "        graph.output(out)\n",
    "        print(\"final graph:\", graph)\n",
    "\n",
    "    # 2. Create an inference session\n",
    "    session = engine.InferenceSession(devices=[device])\n",
    "    model = session.load(graph)\n",
    "\n",
    "    for tensor in model.input_metadata:\n",
    "        print(\n",
    "            f\"name: {tensor.name}, shape: {tensor.shape}, dtype: {tensor.dtype}\"\n",
    "        )\n",
    "\n",
    "    # 3. Execute the graph\n",
    "    ret = model.execute(a, b)[0]\n",
    "    print(\"result:\", ret)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start with regular np arrays and pass them to the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final graph: mo.graph @simple_add_graph(%arg0: !mo.tensor<[1], f32>, %arg1: !mo.tensor<[1], f32>) -> !mo.tensor<[1], f32> attributes {argument_names = [\"input0\", \"input1\"], result_names = [\"output0\"]} {\n",
      "  %0 = mo.chain.create()\n",
      "  %1 = rmo.add(%arg0, %arg1) : (!mo.tensor<[1], f32>, !mo.tensor<[1], f32>) -> !mo.tensor<[1], f32>\n",
      "  mo.output %1 : !mo.tensor<[1], f32>\n",
      "}\n",
      "name: input0, shape: [1], dtype: DType.float32\n",
      "name: input1, shape: [1], dtype: DType.float32\n",
      "result: max.driver.Tensor(DType.float32, (1,))\n",
      "Device(type=gpu,gpu_id=0,target_info(triple=nvptx64-nvidia-cuda,arch=sm_86,features=[])\n"
     ]
    }
   ],
   "source": [
    "input0 = np.array([1.0], dtype=np.float32)\n",
    "input1 = np.array([1.0], dtype=np.float32)\n",
    "ret = add_tensors(input0, input1, my_device)\n",
    "print(ret.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy the result tensor back to CPU and convert back to a np array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device(type=cpu,target_info(triple=x86_64-unknown-linux-gnu,arch=znver3,features=[adx, aes, avx, avx2, bmi, bmi2, clflushopt, clwb, clzero, crc32, cx16, cx8, f16c, fma, fsgsbase, fxsr, invpcid, lzcnt, mmx, movbe, mwaitx, pclmul, pku, popcnt, prfchw, rdpid, rdpru, rdrnd, rdseed, sahf, sha, sse, sse2, sse3, sse4.1, sse4.2, sse4a, ssse3, vaes, vpclmulqdq, wbnoinvd, x87, xsave, xsavec, xsaveopt, xsaves])\n"
     ]
    }
   ],
   "source": [
    "ret_cpu = ret.to(CPU())\n",
    "print(ret_cpu.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.], dtype=float32)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ret_cpu.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
